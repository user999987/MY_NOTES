## C1
(色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂:稍蜷;敲声=沉 闷)， (色泽=浅自;根蒂t硬挺;敲声=清脆) 每对括号内是一条记录

data set: 上面这组记录的集合 

instance/sample: 每个记录关于一个事件或者对象的描述
feature/attribute: 事件或对象某方面的表现或性质

attribute value: 

attribute/sample/input space: 

feature vector: "色泽" "根蒂" "敲声"作为三个坐标轴，则它们张成 一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位 置.空间中的每个点对应一个坐标向量

D = $
{x_1,x_2,...x_m
}$: m 个 instance 的 data set 

$x_i = (x_{i1};x_{i2};...x_{id})$ : 每个 示例由 d 个属性描述(例如上面的西瓜数据使用了 3 个属性)  $x_i$是 d 维 样本空间 X 中的一个 vector

example: instance with label

classification: 预测离散值

regression: 预测连续值

supervised learning: data set with label

unsupervised learning: data set without label

induction: 特殊到一般的"泛化" (generalization)过程，即从具体的事实归结出一般性规 律 

deduction: 一般到特殊的"特化" (specializatio叫过程，即从基础原理推演
出具体状况.例如，在数学公理系镜中，基于一组公理和推理规则推导出与之 相洽的定理

hypothesis space: 色泽 根蒂 敲声 的各种搭配 

version space: 现实问题中我们常面临很大的假设空间?但学习过程是基于 有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与 训练集一致的"假设集合"

```
编号色泽根蒂敲声好瓜
1 青绿蜷缩浊响 是 
2 乌黑蜷缩浊响 是 
3 青绿硬挺清脆 否 
4 乌黑稍蜷沉闷 否
```
inductive bias: (色泽口青绿; 根蒂=蜷缩;敲声=沉闷)这个新收来的瓜，如果我们采用的是"好瓜村(色 泽= *)八(根蒂=蜷缩)八(敲声=*)"，那么将会把新瓜判断为好瓜，而如果采 用了另外两个假设，则判断的结果将不是好瓜. 对于一个具体的学习算法而言?它必须要产生一个模型.这时，学习算 法本身的"偏好"就会起到关键的作用.例如，若我们的算法喜欢"尽可能特 殊"的模型，则它会选择"好瓜件(色泽= *)八(根蒂=蜷缩)八(敲声=浊晌)" ; 但若我们的算法喜欢"尽可能一般"的模型，并且由于某种原因它更"相信" 根蒂，则它会选择"好瓜件(色泽= *) ^(根蒂=蜷缩)八(敲声= *)" .机器学习 算法在学习过程中对某种类型假设的偏好

任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看 似在训练集上"等效"的假设所迷惑，而无法产生确定的学习结果. 什么意思呢, 就是瓜"(色泽=青绿;根蒂口蜷缩;敲声=沉
闷)" ，学得模型时而告诉我们它是好的、时而告诉我们它是不好的，这样的学 习没有意义.

Occam's razor: 一种一般性的原则来引导算法确立 "正确的"偏好 自然科学 研究中最基本的原则，即"若有多个假设与观察一致，则选最简单的那个 具体例子就是 当用上表画图的时候可以有非常多条曲线满足 此时选择最平滑的那条 即最简单的那个 但是奥卡姆剃刀本身存在不同的诠释 .例如对我们已经很熟悉的西瓜问题来说，"假设 1: 好瓜件 (色泽= *) ^(根蒂=蜷缩)八(敲声=浊响)"和假设 2: "好瓜件(色泽= *) ^ (根蒂=蜷缩)八(敲声= *)"这两个假设，哪一个更"简单"呢?这个问题并不 简单，需借助其他机制才能解决.


## C2
error rate: 分类错误的样本数 占 样本总数的 比例

error 误差: 学习器的实际预测输出与样本的真实输出之间的差异

training error训练误差/empirical error 经验误差: 学习器在训练集上的误差称

generalization error泛化误差: 在新样本上的误差 

overfitting过拟合: 学习器把训练样本学得"太 好", 可能巳经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下.  过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一 些针对过拟合的措施;然而必须认识到，过拟合是无法彻底避免的，我们所能做 的只是"缓解'气或者说减小其风险

underfitting欠拟合: 对训练样本的一般性质尚未学好 拟合比较容易克服，例如在决策树学习中扩展分
支、在神经网络学习中增加训练轮数等

### 2.2 评估方法
hold-out留出法: 直接将 data set 划分为 两个互斥的集合 其中一个集合作为训练集 S 另一个作为测试集 T  训练/测试集的划分要尽可能保持数据分布的一致性，避免 困数据划分过程引入额外的偏差而对最终结果产生影响

stratified sampling分层采样: 对 D 进行分层采样而获得含 70% 样本的训练集 S 和含 30% 样本的测试集 T， 若 D 包含 500 个正例、 500 个反例，则分层采样得到的 S 应包含 350 个正例、 350 个反例?而 T 则包含 150 个正例和 150 个反例;若 S、 T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异 而产生偏差.\
注意: 另一个需注意的问题是，即使在给定训练/测试集的样本比例后，仍存在多 种划分方式对初始数据集 D 进行分割.例如在上面的例子中，可以把 D 中的样 本排序，然后把前 350 个E例放到训练集中，也可以把最后 350 个正例放到训 练集中，......这些不同的划分将导致不同的训练/测试集，相应的?模型评估的 结果也会有差别.因此?单次使用留出法得到的估计结果往往不够稳定可靠，在 使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作 为留出法的评估结果.例如进行 100 次随机划分，每次产生一个训练/测试集用 于实验评估， 100次后就得到 100个结果?而留出法返回的则是这 100个结果的 平均.

cross validation: 每次用 k-1 个子集的并集作为训练集?余 F的那个子集作为测试集;这样就可获得 k 组训练/测试集，从而可进行 k次训练和测试? 最终返回的是这 k个测试结果 的均值 k 一般为 10
 ```
 training set                test set
 D1,D2,D3,D4,D5,D6,D7,D8,D9     D10
 D1,D2,D3,D4,D5,D6,D7,D8,D10    D9
 ......                         ...
 D2,D3,D4,D5,D6,D7,D8,D9,D10    D1

 得到平均结果并返回
 ```
 与 留出法相似，将数据集 D 划 分 为 k 个子集同样存在多种 划 分方式.为 减小因样本划分不同而引入的差别， k折交叉验证通常要随机使用不同的划分 重复 p次?最终的评估结果是这 p次 k折交叉验证结果的均值，例如常见的有 "10次 10折交叉验证

 Leave-One-Out留一法:数据集 D 中包含 m 个样本3 若令 k=m  留一法使用的训练集与初始数据集相比只少了一个样本，这就使得 在绝大多数情况下，留一法中被实际评估的模型与期望评估的用 D 训练出的模 型很相似 在数据集比较大时，训练 m 个模型的计算开销可能是难以忍受的(例如数 据集包含 1 百万个样本，则需训练 1 百万个模型)

 bootstrapping有放回采样: 给定包含 m 个样 本的数据集 D ， 我们对它进行采样产生数据集 D': 每次随机从 D 中挑选一个 样本7 将其拷贝放入 DF' 然后再将该样本放回初始数据集 D 中，使得该样本在 下次采样时仍有可能被采到;这个过程重复执行 m 次后?我们就得到了包含 m 个样本的数据集 DF，这就是自助采样的结果 可以做一个简单的估计，样本在 m 次采 样中始终不被采到的概率是 $(1一 \frac {1}{m})^m$ ， 取极限得到 $\frac {1}{e}$ 约等于 0.368

 out-of-bag estimate: 没在训 练集中出现的样本用于测试.这样的测试结果，亦称"包外估计" 

 自助法在数据集较小、难以有效划分训练/测试集时很有用;此外，自助法 能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处. 然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.因此，在初始数据量足够时，留出法和交叉验证法更常用一些.

 parameter tuning: 调参 对每种参数配置都 训练出模型，然后把对应最好模型的参数作为结果.这样的考虑基本是正确的， 但有一点需注意:学习算法的很多参数是在实数范围内取值，因此，对每种参数 配置都训练出模型来是不可行的.现实中常用的做法?是对每个参数选定一个 范围和变化步长，例如在 [0， 0.2] 范围内以 0.05 为步长，则实际要评估的候选参 数值有 5 个，最终是从这 5 个候选值中产生选定值 在 模型选择完成后，学习算法和参数配置己选定，此时应该用数据集 D 重新训练 模型.这个模型在训练过程中使用了所有 m 个样本，这才是我们最终提交给用 户的模型.

我们通常把学得模型在实际使用中遇到的数据称为测 试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为"验 证集" (validation set). 例如，在研究对比不同算法的泛化性能时，我们用测试 集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分 为训练集和验证集，基于验证集上的性能来进行模型选择和调参.

### 2.3 性能度量
performance measure性能度量: 衡量模型泛化能力的评价标准. 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往 往会导致不同的评判结果;这意味着模型的"好坏"是相对的，什么样的模型 是好的?不仅取决于算法和数据，还决定于任务需求.

$D = \{(x_1,y_1),(x_2,y_2),...(x_m,y_m)\}$

回归任务最常用的性能度量是 均方误差 mean squared error

Precision: 预测某种状态正确的样本数量/预测为某种状态的所有样本数量 \
精确率 = 真正例 / (真正例 + 假正例)

Recall: 预测某种状态正确的样本数量/实际某种状态的样本数量 \
召回率 = 真正例 / (真正例 + 假负例)

precision 和 recall 一个高另一个就低

Break-Event Point: precision=recall 时的一个取值基于 BEP 的比较可以认为 学习器 A 是否优于 B

F1: 比 BEP 更常用的 度量 基于 precision 和 recall 的 harmonic mean 定义的 $\frac {1}{F1} = \frac{1}{2} (\frac{1}{P} + \frac{1}{R})$

Fβ: weighted harmonic mean $\frac {1}{Fβ} = \frac{1}{1+β^2} (\frac{1}{P} + \frac{β^2}{R})$  β > 1, Recall has larger influence, β < 1, precision has larger influence

ROC receiver operating characteristic: 用于评估二元分类模型性能的一种图形表示方法。它显示了在不同分类阈值下，模型的真正例率（True Positive Rate）与假正例率（False Positive Rate）之间的权衡关系\
比较 ROC 曲线下的面积，即 AUC (Area Under ROC Curve) 用来比较两个学习器的 优劣好坏 \
很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与 一个分类阔值( threshold)进行比较，若大于|词值则分为正类，否则为反类。 \
根据这个实值或 概率预测结果，我们可将测试样本进行排序，"最可能"是正例的排在最前面，"最不可能"是正例的排在最后面。更重视 Precision ，则可选择排序中靠前的位置进行截断;若更重视 Recall ，则可选择靠后的位置进行截断。因此，排序本身的质量好坏，体现了综 合考虑学习器在不同任务下的"期望泛化性能"的好坏，或者说"一般情况 下"泛化性能的好坏. ROC 曲线则是从这个角度出发来研究学习器泛化性能 的有力工具.

AUC: ROC下面的面积

代价曲线没咋看懂 公式

### 2.4 比较检验
如何比较 统计假设检验 hypothesis test