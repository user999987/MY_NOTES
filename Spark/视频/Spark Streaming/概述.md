## Spark Streaming
用于流式数据的处理。 支持 Kafka，Flume，Twitter，ZeroMQ 和 简单的 TCP scoket。 数据输入后可以用 Spark 的高度抽象原语言 如 map，reduce，join，window 等进行计算，结果可以保存在如 HDFS，数据库等地方。

每个 time interval 读一波数据 封装成 RDD\
DStream (Discretized Stream)
随着时间推移而收到的数据的序列。在内部，每个时间区间 (x seconds) 收到的数据都作为 RDD 存在， DStream 就是由这些 RDD 组成的序列

## 2 approaches to pull data from Kafka
Spark streaming 有两种方式从 Kafka 读数据
1. Receiver 和 Kafka 高阶 API
2. Direct 

https://segmentfault.com/a/1190000019494386