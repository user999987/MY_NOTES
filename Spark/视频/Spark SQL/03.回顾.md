SparkSQL 内置一个 Hive
两个 spark shell 会报错 因为 Hive 默认数据存在 Derby 而 Derby 是单用户模式
工作中基本不会用 都是自己改 conf 连接外部 Hive
在 Hive shell 跑一个命令大概30s Spark 跑同样的命令 大概3-4s


MapReduce -> Spark Core
Hive -> Spark SQL

## Spark SQL
1. 什么是 Spark SQL
Spark 用来处理结构化数据的一个模块。提供了2个编程抽象接口：DataFrame 和 DataSet (Python没有这个)，并且作为分布式 SQL 查询引擎的作用。
Hive 将 Hive SQL 转换成 MapReduce 提交到集群上执行，简化了编写 MapReduce 的程序的复杂性，但是有点慢。Spark SQL 转换成 RDD 提交到 集群运行 加快了速度。
2. Spark SQL 特点
* Integrated\
Seamlessly mix SQL queries with Spark programs.
Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.
* Uniform Data Access\
Connect to any data source the same way.
DataFrames and SQL provide a common way to access a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. You can even join data across these sources.
* Hive Integration\
Run SQL or HiveQL queries on existing warehouses.
Spark SQL supports the HiveQL syntax as well as Hive SerDes and UDFs, allowing you to access existing Hive warehouses.
* Standard Connectivity\
Connect through JDBC or ODBC.
A server mode provides industry standard JDBC and ODBC connectivity for business intelligence tools.
3. 什么是 DataFrame


## DataFrame 介绍


## 数据 加载和保存
### 通用
1. Read
* spark.read.csv
* spark.read.format
2. Write
* df.write.jdbc
* df.write.format
文件保存选项 - 当保存路径或者表已经存在时
* Append - 追加
* Overwrite - 覆盖
* ErrorIfExists - 报错
* Ignore - 忽略当前保存操作
df.write.mode(SaveMode.Append).save('...')
3. 默认数据源
```python
df = spark.read.load("blbl.quaret")
```
不用指定 read.csv or read.json 可以修改配置项 spark.sql.srouces.default 来修改默认数据源 格式

### JSON 和 MySQL
JSON 每行一个 Json 串
MySQL 读写
```python
>>>dataframe = spark.read.format("jdbc").option("url", "jdbc:mysql://hadoop102/3306").option("dbtable", "emp").option("user", "root").option("password", "12345").load()
```

### Hive
内置基本没人用 通过 conf 配置连接外部 Hive
```scala
val spark: SparkSession = SparkSession
    .builder()
    .enableHiveSupport()
    .master("local[*]")
    .appName("SQLTest")
    .getOrCreate()
```

## Spark SQL 应用

rdd.toDF <=> dataFrame.rdd

### UDF - User Defined Function
spark.udf.register(name, f, returnType=None)
```python
>>> strlen = spark.udf.register("stringLengthString", lambda x: len(x))
>>> spark.sql("SELECT stringLengthString('test')").collect()
[Row(stringLengthString(test)='4')]
```