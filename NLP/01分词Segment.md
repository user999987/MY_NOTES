# 中文分词
自中文自动分词被提出以来, 历经将近 30 年的探索, 提出了很多方法, 可主要归纳为:
* 规则分词
* 统计分词
* 混合分词(规则+统计)

## 规则分词
* 正向最大匹配法
* 逆向最大匹配法
* 双向最大匹配法

基于规则的分词，一般都较为简单高效，但是词典的维护是一个很庞大的工程。 在网络发达的今天，网络新词层出不穷，很难通过词典覆盖到所有词。

## 统计分词
* 统计分词的主要思想: 把每个词看做是由词的最小单位的各个字组成的, 如果相连的字在不同的文本中出现的次数越多, 就证明这相连的字很可能就是一个词. 因此我们就可以利用 字与字相邻出现的频率 来反应 成词的可靠度, 统计语料中相邻共现的各个字的组合的频度, 当组合频度高于某一个临界值时,便可以认为此字组成会构成一个词语
* 基于统计的分词, 一般要做如下两步操作:
    1. 建立统计语言模型
    2. 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就用到了统计学习算法， 如隐式马尔科夫(HMM)、条件随机场(CRF)等

## 混合分词
事实上，目前不管是基于规则的算法、基于 HMM、CRF 或者 deep learning 等的方法， 其分词效果在具体任务中，其实差距并没有那么明显。

在实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。最常用的方式就是先基于词典的方式进行分词， 然后再用统计方法进行辅助。如此，能在保证词典分词准确率的基础上，对未登录词和歧义词有较好的识别。

jieba 分词工具就是基于这种方法的实现
