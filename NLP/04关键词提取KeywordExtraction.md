# 关键词提取
关键词是代表文章重要内容的一组词。对文本聚类、分类、自动摘要等起重要作用。此外，它还能使人们便捷地浏览和获取信息。 类似于其他的机器学习方法，关键词提取算法一般也可以分为有监督和无监督两类
* 有监督的关键词提取方法主要是通过分类的方式进行，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中每个词的匹配程度， 以类似打标签的方式，达到关键词提取的效果。
    * 有监督的方法能够获取到较高的精度，但缺点是需要大批量的标注数据，人工成本过高
    * 另外，现在每天的信息增加过多，会有大量的新信息出现，一个固定的词表有时很难将新信息的内容表达出来， 但是要人工维护这个受控的词表却要很高的人力成本，这也是使用有监督方法来进行关键词提取的一个比较大的缺陷
* 无监督的方法对数据的要求比较低，既不需要一张人工生成、维护的词表，也不需要人工标准语料辅助进行训练。 因此，这类算法在关键词提取领域的应用更受到大家的青睐。
    * TF-IDF 算法
    * TextRank 算法
    * 主题模型算法
        * LSA
        * LSI
        * LDA

# TF-IDF算法 Term Frequency- Inverse Document Frequency
TF-IDF 算法(Term Frequency-Inverse Document Frequency，词频-逆文档频次算法)，是一种基于统计的计算方法， 常用于评估一个文档集中一个词对某份文档的重要程度。这种作用显然很符合关键字抽取的需求，一个词对文档越重要，那就越可能 是文档对的关键词，常将 TF-IDF 算法应用于关键词提取中。
* TF 算法: 统计一个词在一篇文档中出现的频次，其基本思想是，一个词在文档中出现的次数越多，则其对文档的表达能力就越强
* IDF 算法: 统计一个词在文档集的多少个文档中出现，其基本思想是，如果一个词在越少的文档中出现，则其对文档的区分能力也就越强

## TF
$tf_{ij} = \frac{n_{ij}}{\sum_k n_{kj}}$\
$tf(word) = \frac{word在文档中出现的次数}{文档总词数}$
* $n_{ij}$表示i在文档j中出现的频次
* ${\sum_k n_{kj}}$ 是统计文档中每个词出现次数的总和, 也就是文档的总词数
## IDF
$idf_i=log(\frac{|D|}{1+|D_i|})$
* $|D|$为文档集中的总文档数
* $|D_i|$为文档集中出现词 𝑖 的文档数量。分母加 1 是采用了拉普拉斯平滑， 避免有部分新的词没有在语料库中出现过而导致分母为零的情况出现，增强算法的健壮性
## TF-IDF
$tf * idf(i,j) = \frac{n_{ij}}{\sum_k n_{kj}} * log(\frac{|D|}{1+|D_i|})$

TF-IDF 算法也有很多变种的加权方法。传统的 TF-IDF 算法中，仅考虑了词的两个统计信息(出现频次、在多少个文档出现)， 因此，其对文本的信息利用程度显然也是很少的。\
除了上面的信息外，在一个文本中还有许多信息能够对关键词的提取起到很好的知道作用，例如每个词的词性、出现的位置等。\
在某些特定的场景中，如在传统的 TF-IDF 基础上，加上这些辅助信息，能对关键词提取的效果起到很好的提高作用。

# TextRank算法
TextRank 算法的基本思想来源于 Google 的 PageRank 算法. PageRank 算法是一种网页排名算法，其基本思想有两条:
1. 链接数量
2. 链接质量
    * 一个网页被一个越高权值的网页链接，也能表明这个网页越重要