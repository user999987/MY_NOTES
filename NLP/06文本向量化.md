# 1. 概述
## 1.1 词汇表征
文本向量化: 
* 将文本表示成计算机可识别的实数向量， 根据粒度大小不同，可将文本特征表示分为字、词、句子、篇章几个层次
* 一般称为词嵌入(word embedding)方法，词嵌入这个说法很形象，就是把文本中的词嵌入到文本空间中， 用一个向量来表示词

文本向量化方法:
* 离散词向量表示
    * 基于规则、统计
        * 词集模型(set of word)
            * One-Hot encoding
            * 统计各词在句子中是否出现
        * 词袋模型(bag of word)
            * 统计各词在句子中出现的次数
        * Bi-gram、N-gram
        * TF-IDF
            * 统计各词在文档中的 TF-IDF 值(词袋模型 + IDF 值)
        * 共现矩阵
* 分布式词向量表示
    * 基于神经网络的词嵌入
        * word2vec
        * doc2vec
        * str2vec
## 1.2 词向量模型建立词汇表
NLP 相关任务中最常见的第一步是创建一个 词表库 并把每个词顺序编号。
# 2. 离散表示
文本向量化离散表示是一种基于规则和统计的向量化方式，常用的方法包括 词集模型 和 词袋模型， 都是基于词之间保持独立性、没有关联为前提，将所有文本中单词形成一个字典，然后根据字典来统计单词出现频数
* 词集模型:
    * 统计各词在句子中是否出现
    * 例如 One-Hot Representation，只要单个文本中单词出现在字典中，就将其置为 1，不管出现多少次
* 词袋模型:
    * 统计各词在句子中出现的次数
    * 只要单个文本中单词出现在字典中，就将其向量值加 1，出现多少次就加多少次

其基本的特点是忽略了文本信息中的语序信息和语境信息，仅将其反映为若干维度的独立概念， 这种情况有着因为模型本身原因而无法解决的问题，比如主语和宾语的顺序问题， 词袋模型天然无法理解诸如“我为你鼓掌”和“你为我鼓掌”两个语句之间的区别

## 2.1 One-Hot Representation
假设我们有一个包括 10000 个单词的词汇表， 现在需要用 one-hot 方法来对每个单词进行编码。以上面那句 “I want a glass of orange ____.” 为例， 假设 I 在词汇表中排在第 3876 个，那么 I 这个单词的 one-hot 表示就是一个长度为 10000 的向量， 这个向量在第 3876 的位置上为 1 ，其余位置为 0，其余单词同理，每个单词都是茫茫 0 海中的一个 1

可见 one-hot 词汇表征方法最后形成的结果是一种稀疏编码结果，在深度学习应用于 NLP 任务之前， 这种表征方法在传统的 NLP 模型中已经取得了很好的效果。但是这种表征方法有两个缺陷: 一是容易造成维数灾难，10000 个单词的词汇表不算多，但对于百万级、千万级的词汇表简直无法忍受。 第二个则在于这种表征方式不能很好的词汇与词汇之间的相似性，比如上述句子， 如果我们已经学习到了 “I want a glass of orange juice.”，但如果换成了 “I want a glass of apple ____.”， 模型仍然不会猜出目标词是 juice。因为基于 one-hot的表征方法使得算法并不知道 apple 和 orange 之间的相似性， 这主要是因为任意两个向量之间的内积都为零，很难区分两个单词之间的差别和联系。

## 2.2 Bag of Word
* 对于句子、篇章，常用的离散表示方法是词袋模型，词袋模型以 One-Hot 为基础，忽略词表中词的顺序和语法关系， 通过记录词表中的每一个词在该文本中出现的频次来表示该词在文本中的重要程度，解决了 One-Hot 未能考虑词频的问题
* 词袋(Bag Of Word) 模型是最早的以词语为基本单元的文本向量化方法。词袋模型，也称为计数向量表示(Count Vectors). 文档的向量表示可以直接使用单词的向量进行求和得到
## 2.3 Bi-gram N-gram
Bi-gram 将相邻两个词编上索引，N-gram 将相邻 N 个词编上索引
```
基于文档中出现的单词
{
   "John likes": 1,
   "likes to": 2,
   "to watch": 3,
   "watch movies": 4,
   "Mary likes": 5,
   "likes too": 6,
   "John also": 7,
   "also likes": 8,
   "watch football": 9,
   "football games": 10,
}
```

## 2.4 TF-IDF
