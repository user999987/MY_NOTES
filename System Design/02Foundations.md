## 壹 Load Balancer
Load balancer add failover to the web tier by distributing the traffic to different servers. Yet there is possibility that LB itself goes down, in that case, we can have high availability pair of load balancers or DNS based failover which means multiple load balancers are assigned to single domain name.

## 贰 Master/Slave
A master database generally only supports write operations. A slave database gets copies of the data from the master DB and only supports read operations.

Advantages of database replication:
- better performance for w/r
- reliability: if one of your DB is destroyed by a natural disaster, no need worry about data loss.
- high availability: website can remains in operation even if a database is offline

For Master/Slave mode, there is a obvious issue, synchronization latency, that would lead to data consistency issues. \
Solution options:
- Synchronous replication: master node waits for acknowledgments from all slave nodes before returning a success. This will add significant latency to write requests and may not be feasible in systems with high write throughput.
- Asynchronous replication with lag monitoring: Asynchronous replication master node do not wait for slave nodes confirmation. Lag monitoring is used to track the time delay, if lag time increases, it indicates slave server is experiencing performance issue.
- multi-master replication: but may cause conflicts

Database availability:
- if only one slave database available and it goes offline, read operations will be directed to the master temporarily. multiple slave nodes, traffic will be routed to healthy one.
- if master database goes offline, data loss may happen. WAL could be a solution. (Each node has its own WAL, as its name stated, the data is written into log before to the database, binlog is other thing. It is generated by DB after DB write operation succeed.)

## 叁 Cache Strategy
Cache strategy:
1. Cache-Aside: App responsible for loading and storing data
    - App->cache:
        - hit
        - no hit->read data from DB-> write data to cache
    - Pros
        1. usually general purpose and work best for read-heavy workloads. It is resilient to cache failures because if cache cluster goes down, the system can still operate by going directly to the database. 
        2. data model in cache can be different that the data model in database
    - Cons: most common write strategy in cache-aside is write data to the database directly which will result in data inconsistency
        1. Set TTL 
        2. invalidate the cache entry(when write happens or pub-sub delete cache entry when receive write event)
2. Read-Through:
    - App->cache:
        - hit
        - no hit->read from database and store in cache
    - Pros:
        1. works best for read-heavy workloads when the same data is request many times.(for example a news story)
    - Cons:
        1. first hit will always be miss. Deal with this by `warming` or `pre-heating` the cache by issuing queries.
        2. data inconsistency may also exist solution lies in the write strategy
3. Write-Through
    - App->cache->Database(Database is updated by cache)
    - Pros:
        1. no more inconsistency
    - Cons:
        1. extra write latency
4. Write-Around: data is written directly to the database and only the data that is read makes it way into the cache
    - Write-around can be combine with read-through and provides good performance in situations where data is written once and read less frequently or never like real-time logs or chatroom messages. Can be combined with cache-aside as well.
5. Write-Back or Write-Behind: the application writes data to the cache which stores the data and acknowledges to the application immediately. Then later, the cache writes the data back to the database.
    - Pros:
        1. improve the write performance and good for write-heavy workloads. When combined with read-through, it works good for mixed workloads, where the most recently updated and accessed data is always available in cache.
        2. reduce the overall writes to DB if batching or coalescing is supported.
        3. Redis for cache-aside and write-back to better absorb spikes
    - Cons:
        1. if cache failure, data may be permanently lost
### Summary
1. High-read, Low-write: read-through or cache-aside.
2. High-write, Low-read: write-through, reduce the risk of data loss
3. High-write, High-read: write-back provides high write performance and reduce the load on the backend store.
4. where data is infrequently accessed, write-around is a good choice. This avoids unnecessary caching and reduces the memory usage of the cache.

## 肆 System Design Question: Any Optimization?
Any optimization? Can introduce logging, metrics and automation(like CI/CD)

## 伍 Sharding
Sharding use hash function to find the corresponding shard(usually it will be a sharding key. like in Users table, user_id is sharding key)

Celebrity Problem, also called hotsopt key problem. Excessive access to a specific shard could cause server overload. Possible solution is allocate a shard for each celebrity.

Once a database has been sharded across multiple servers, it is hard to perform join operations across database shards. A common workaround is to de-normalize the database so that queries can be performed in a single table. \
De-normalization is the process of adding precomputed redundant data to an otherwise normalized relational database to improve read performance of the database. Normalizing a database involves removing redundancy so only a single copy exists of each piece of information.

## 陆 Summary
1. Web tier stateless so LB can distribute traffic to any server
2. Redundancy at every tier
3. Cache data as much as you can (unless you do not really access them)
4. Support multiple data centers
5. Static assets in CDN
6. Scale data tier by sharding
7. Split tiers into individual services
8. Monitor (metrics) your system and use CI/CD

